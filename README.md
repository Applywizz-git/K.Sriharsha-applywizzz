# Portfolio â€“ K SRIHARSHA

A modern, animated, and immersive portfolio website built with **React, Vite, TailwindCSS, Framer Motion, AOS.js, and Swiper.js**.  
The site showcases the professional experience, projects, skills, certifications, and education of **K SRIHARSHA**.

---

## ðŸ“‚ Sections

### ðŸ”¹ Header
- Sticky, glassmorphism background
- Logo + Navigation (Home, About, Experience, Projects, Skills, Certification, Education, Contact)
- Mobile-friendly overlay menu

### ðŸ”¹ Hero
- Typewriter headline: *"Building Scalable Web Apps with Modern Tech"* â†’ cycles (React, Node, TypeScript, Cloud)
- Left: intro + call-to-action buttons
- Right: profile photo (animated entrance)
- Background: parallax image/video

### ðŸ”¹ About
**Summary:**
Data Engineer with 3 years of experience designing and optimizing scalable ETL pipelines, data lakes, and cloud architectures across AWS, Azure, and hybrid environments. Expert in Spark, PySpark, Scala, Kafka, Airflow, Databricks, SQL, and Python for real-time and batch data processing, warehouse migrations (Synapse, Redshift, Snowflake), and BI integration with Power BI and Tableau. Skilled in data modeling, governance, quality, and high-throughput secure data solutions, enabling actionable insights for financial, operational, and enterprise analytics.

### ðŸ”¹ Experience
**Data Engineer | Credit Suisse | New York, USA | Aug 2023 â€“ Present**  
- Migrated on-prem warehouses to Synapse (35% faster queries)  
- Built Spark workloads (50% throughput gain)  
- Kafka real-time ingestion (<2s latency)  
- Automated monitoring with Airflow (30% less downtime)  
- Achieved 99% dataset accuracy  

**Data Analyst | Hewlett Packard | India | Oct 2020 â€“ Nov 2021**  
- Built Power BI dashboards (30% faster decision-making)  
- Optimized SQL Server queries (25% faster reporting)  
- Processed large datasets with Hadoop + Spark (40% faster)  
- Automated ETL & reports (manual effort cut 30%)  

### ðŸ”¹ Projects
1. **Real-Time Loan Analytics Platform** â€“ Azure Data Factory, Databricks, Synapse, Kafka, Spark â†’ real-time analytics  
2. **Cloud-Based Data Warehouse Migration & Optimization** â€“ AWS Glue, Lambda, S3, EMR â†’ 35% faster ETL, 25% cost savings  
3. **Enterprise Data Lake & Streaming Analytics Solution** â€“ Spark, Hive, Airflow DAGs, Kafka streaming â†’ reliable low-latency insights  

### ðŸ”¹ Skills
- **Programming:** Python, SQL, T-SQL, PySpark, Bash, Shell  
- **Big Data:** Spark, Hadoop, Hive, Pig, Sqoop, Flume, Oozie, Kafka, Airflow, Delta Lake, dbt, Informatica, SSIS, Talend  
- **Cloud:** AWS (S3, EMR, Redshift, Glue, Lambda, RDS, DynamoDB, Athena, CloudWatch, SNS, SQS, Kinesis), Azure (ADF, Synapse, Databricks, Data Lake Gen2, Azure SQL, Functions, Logic Apps, Key Vault), GCP (BigQuery, Dataflow, Pub/Sub, Cloud Storage)  
- **Databases:** MySQL, PostgreSQL, MongoDB, HBase, SQL Server, Snowflake, Teradata, Oracle, Cosmos DB  
- **Visualization:** Power BI, Tableau, Looker, Qlik Sense, Matplotlib, Seaborn, Plotly, Pandas Profiling  
- **ML/Data Science:** Pandas, NumPy, SciPy, Scikit-learn, TensorFlow, PyTorch  
- **DevOps:** Docker, Kubernetes, Jenkins, Git, GitHub Actions, Terraform, Helm  
- **Tools & OS:** Linux, Windows, MacOS, VS Code, PyCharm, Jupyter Notebook  

### ðŸ”¹ Certifications
- IBM AI Practitioner â€“ Coursera  
- Google Data Analytics Professional Certificate â€“ Coursera  
- Microsoft Azure Data Engineer Associate â€“ LinkedIn Learning  
- Data Engineering with Google Cloud Professional Certificate â€“ Coursera  

### ðŸ”¹ Education
- Master of Computer Science | University of Central Missouri | Warrensburg, MO  

### ðŸ”¹ Contact
- ðŸ“ Aubrey, TX  
- ðŸ“ž +1 (913)-201-6393  
- ðŸ“§ [harshas4411@gmail.com](mailto:harshas4411@gmail.com)  
- ðŸŒ [Portfolio](https://ksriharsha-applywizz.vercel.app/)  

---

## ðŸš€ Tech Stack
- React + Vite  
- Tailwind CSS  
- Framer Motion  
- AOS.js  
- Swiper.js  

---

## ðŸ›  Setup & Installation
```bash
# Clone repo
git clone <repo-url>

# Move into folder
cd portfolio

# Install dependencies
npm install

# Start development server
npm run dev

# Build for production
npm run build
